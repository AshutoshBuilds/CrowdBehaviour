{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5935ce5e-ae8c-4df8-97d5-927363c70d27",
   "metadata": {},
   "source": [
    "### IMPORT AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03268b69-660c-44fc-a62f-26e8aa9fc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35aaa168-ff38-4bd5-ac07-95d75a3bc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5011c72-b147-40f4-8d0f-2fc10b968bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "653e4dad-a32a-4327-aff5-8410308ee126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user opencv-python albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cc82dfa-c151-4cf4-9173-6c346f2f1e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge opencv albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bea86ce-bdaf-4a0b-94d2-1518b40dbaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available? {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    # Get the number of GPUs\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    # Get the name of the current GPU\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7158b937-649b-44e5-a12d-385462c57d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.8.0+cpu.html\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\srish\\anaconda3\\lib\\site-packages (2.1.2+pt28cpu)\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\srish\\anaconda3\\lib\\site-packages (0.6.18+pt28cpu)\n",
      "Requirement already satisfied: torch-cluster in c:\\users\\srish\\anaconda3\\lib\\site-packages (1.6.3+pt28cpu)\n",
      "Requirement already satisfied: torch-spline-conv in c:\\users\\srish\\anaconda3\\lib\\site-packages (1.2.2+pt28cpu)\n",
      "Requirement already satisfied: scipy in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from scipy->torch-sparse) (2.1.3)\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\srish\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (2.1.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\srish\\anaconda3\\lib\\site-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from aiohttp->torch-geometric) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srish\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\srish\\anaconda3\\lib\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "PyTorch Geometric installed and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch Geometric dependencies\n",
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "PYTORCH_VERSION = format_pytorch_version(TORCH_version)\n",
    "CUDA_VERSION = torch.version.cuda.replace('.','') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-{PYTORCH_VERSION}+{CUDA_VERSION}.html\n",
    "!pip install torch-geometric\n",
    "\n",
    "# --- Now, you can successfully import the GNN components ---\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"PyTorch Geometric installed and imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac585bed-f065-4759-a561-88a287397591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7672571-d935-4bdf-8932-0a8c1c359d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For data handling and processing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3181e4a-3a03-4ae0-82fb-918ad5cfffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model backbones\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    print(\"timm not found. Installing...\")\n",
    "    !pip install timm==0.9.12\n",
    "    import timm\n",
    "\n",
    "# For Graph Neural Network components\n",
    "try:\n",
    "    from torch_geometric.nn import GATConv\n",
    "    from torch_geometric.data import Data, Batch\n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric not found. Installing...\")\n",
    "    # Handle installation based on PyTorch and CUDA versions\n",
    "    pytorch_version = torch.__version__\n",
    "    cuda_version = torch.version.cuda\n",
    "    if cuda_version:\n",
    "        cuda_version_str = \"cu\" + cuda_version.replace('.', '')\n",
    "    else:\n",
    "        cuda_version_str = \"cpu\"\n",
    "    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-{pytorch_version}+{cuda_version_str}.html\n",
    "    !pip install torch-geometric\n",
    "    from torch_geometric.nn import GATConv\n",
    "    from torch_geometric.data import Data, Batch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93f623-bee1-404c-ab13-330b42622d89",
   "metadata": {},
   "source": [
    "### DUMMY DATA GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e3cf38e-0f6a-4e9a-9020-a362453a0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_dummy_data():\n",
    "#     \"\"\"\n",
    "#     Creates dummy video files and annotation CSVs to make the script runnable.\n",
    "#     \"\"\"\n",
    "#     print(\"Generating dummy data for demonstration...\")\n",
    "#     base_dir = 'video-anomaly-detection'\n",
    "#     data_dir = os.path.join(base_dir, 'data')\n",
    "#     video_dir = os.path.join(data_dir, 'dummy_videos')\n",
    "    \n",
    "#     os.makedirs(video_dir, exist_ok=True)\n",
    "#     os.makedirs(os.path.join(base_dir, 'weights'), exist_ok=True)\n",
    "\n",
    "#     dummy_annotations = {'video_path': [], 'label': []}\n",
    "#     for i in range(20): # Create 20 dummy videos for a reasonable dataset size\n",
    "#         path = f'dummy_videos/video_{i}.avi'\n",
    "#         dummy_annotations['video_path'].append(path)\n",
    "#         dummy_annotations['label'].append(np.random.choice([\"Normal\", \"Panic\", \"Violent\"]))\n",
    "        \n",
    "#         # Create a short, random-noise video file\n",
    "#         video_full_path = os.path.join(data_dir, path)\n",
    "#         fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#         out = cv2.VideoWriter(video_full_path, fourcc, 25.0, (256, 256))\n",
    "#         for _ in range(125): # 5 seconds at 25 fps\n",
    "#             frame = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "#             out.write(frame)\n",
    "#         out.release()\n",
    "\n",
    "#     annotations_df = pd.DataFrame(dummy_annotations)\n",
    "    \n",
    "#     # Split into train and val and save CSVs\n",
    "#     train_df = annotations_df.sample(frac=0.8, random_state=42)\n",
    "#     val_df = annotations_df.drop(train_df.index)\n",
    "    \n",
    "#     train_df.to_csv(os.path.join(data_dir, 'train_annotations.csv'), index=False)\n",
    "#     val_df.to_csv(os.path.join(data_dir, 'val_annotations.csv'), index=False)\n",
    "    \n",
    "#     print(\"Dummy data generation complete.\")\n",
    "#     print(f\"Train set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcfb2b8d-57e9-4dfb-b5bc-1a2771cbafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder you extracted from movies.rar\n",
    "YOUR_DATASET_PATH = \"C://Users//srish//Downloads//movies\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c25df2b4-a91b-46f5-b4e5-d42ee4dfb41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C://Users//srish//Downloads//movies'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5126d3ce-f71d-479f-9b86-33c1d3a08d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_annotations_from_nested_folders(dataset_root_path):\n",
    "    \"\"\"\n",
    "    Scans a directory with a nested structure like '1/Violence/*.avi',\n",
    "    creates video annotations, and splits them into train/val CSVs.\n",
    "    \"\"\"\n",
    "    print(f\"Scanning for videos in nested folders under: {dataset_root_path}\")\n",
    "\n",
    "    # --- Setup output directories ---\n",
    "    base_dir = 'video-anomaly-detection'\n",
    "    data_dir = os.path.join(base_dir, 'data')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, 'weights'), exist_ok=True)\n",
    "\n",
    "    video_annotations = {'video_path': [], 'label': []}\n",
    "    \n",
    "    # --- Walk through the nested directory structure ---\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        raise ValueError(f\"The provided path is not a directory: {dataset_root_path}\")\n",
    "\n",
    "    for numbered_folder in sorted(os.listdir(dataset_root_path)):\n",
    "        numbered_folder_path = os.path.join(dataset_root_path, numbered_folder)\n",
    "        \n",
    "        if os.path.isdir(numbered_folder_path):\n",
    "            for label_folder in os.listdir(numbered_folder_path):\n",
    "                label_folder_path = os.path.join(numbered_folder_path, label_folder)\n",
    "                \n",
    "                if os.path.isdir(label_folder_path):\n",
    "                    for video_file in os.listdir(label_folder_path):\n",
    "                        if video_file.lower().endswith(('.avi', '.mp4', '.mov')):\n",
    "                            relative_video_path = os.path.join(numbered_folder, label_folder, video_file)\n",
    "                            video_annotations['video_path'].append(relative_video_path)\n",
    "                            video_annotations['label'].append(label_folder.lower())\n",
    "\n",
    "    if not video_annotations['video_path']:\n",
    "        raise ValueError(f\"No videos found in the expected nested structure under {dataset_root_path}.\")\n",
    "\n",
    "    annotations_df = pd.DataFrame(video_annotations)\n",
    "    annotations_df = annotations_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_df, val_df = train_test_split(annotations_df, test_size=0.2, random_state=42, stratify=annotations_df['label'])\n",
    "\n",
    "    train_csv_path = os.path.join(data_dir, 'train_annotations.csv')\n",
    "    val_csv_path = os.path.join(data_dir, 'val_annotations.csv')\n",
    "    \n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "    \n",
    "    print(\"Annotation files created successfully from nested folders.\")\n",
    "    print(f\"Total videos found: {len(annotations_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2c49a9-8369-4b19-9dbb-8ef4245c68a0",
   "metadata": {},
   "source": [
    "### DATASET CLASS DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86841f35-422d-4f02-a84f-e9cf95bd4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading, sampling, and transforming video clips.\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file, data_root, num_frames=16, frame_size=(224, 224), fps=16, is_train=True):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.data_root = data_root\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.target_fps = fps\n",
    "        self.labels_map = {\"Normal\": 0, \"Panic\": 1, \"Violent\": 2}\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Define separate augmentations for training and validation\n",
    "        if is_train:\n",
    "            self.transform =A.Compose([\n",
    "                A.RandomResizedCrop(size=self.frame_size, scale=(0.8, 1.0)),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.8),\n",
    "                A.GaussNoise(p=0.2),\n",
    "                A.CoarseDropout(max_holes=8, max_height=24, max_width=24, p=0.5),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(height=self.frame_size[0], width=self.frame_size[1]),\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.data_root, self.annotations.iloc[idx, 0])\n",
    "        label_str = self.annotations.iloc[idx, 1]\n",
    "        label = self.labels_map[label_str]\n",
    "\n",
    "        frames, frame_diffs = self._load_video_clip(video_path)\n",
    "\n",
    "        # Apply augmentations to all frames\n",
    "        transformed_frames = torch.stack([self.transform(image=frame)[\"image\"] for frame in frames])\n",
    "        \n",
    "        # Normalize frame differences separately\n",
    "        frame_diffs = torch.from_numpy(frame_diffs).float().permute(0, 3, 1, 2) # T,H,W,C -> T,C,H,W\n",
    "        # Simple normalization for diffs\n",
    "        frame_diffs = frame_diffs / 255.0\n",
    "\n",
    "        return transformed_frames, frame_diffs, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    def _load_video_clip(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(f\"Cannot open video file: {video_path}\")\n",
    "\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Uniformly sample indices across the video\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "        frames, frame_diffs = [], []\n",
    "        last_frame_gray = None\n",
    "\n",
    "        for i in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # If reading fails, duplicate the last valid frame\n",
    "                if frames:\n",
    "                    frame = frames[-1].copy()\n",
    "                else: # Very rare case of video with 0 frames\n",
    "                    frame = np.zeros((self.frame_size[0], self.frame_size[1], 3), dtype=np.uint8)\n",
    "            \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (self.frame_size[1], self.frame_size[0]))\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Compute frame difference (simple motion cue)\n",
    "            current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "            if last_frame_gray is not None:\n",
    "                diff = cv2.absdiff(current_frame_gray, last_frame_gray)\n",
    "                frame_diffs.append(np.stack([diff]*3, axis=-1)) # Make it 3-channel\n",
    "            last_frame_gray = current_frame_gray\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "        # Ensure frame_diffs list has the correct length\n",
    "        if not frame_diffs: # If only one frame was loaded\n",
    "            frame_diffs.append(np.zeros_like(frames[0]))\n",
    "        while len(frame_diffs) < self.num_frames:\n",
    "            frame_diffs.append(np.zeros_like(frame_diffs[-1])) # Pad with zero motion\n",
    "\n",
    "        return np.array(frames), np.array(frame_diffs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bc481-073f-4ee1-a5b7-56886fea6b30",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe586bca-52ee-44c6-8345-17d0bf743883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalGNN(nn.Module):\n",
    "    def __init__(self, num_frames=16, num_classes=3, feature_dim=256, gnn_layers=2, num_heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # --- Appearance Stream ---\n",
    "        self.cnn_backbone = timm.create_model('convnext_tiny', pretrained=True, features_only=True, out_indices=[2])\n",
    "        # ConvNeXt-T stage 2 has 384 channels, feature map size 14x14 for 224 input\n",
    "        self.cnn_feature_proj = nn.Conv2d(384, feature_dim, kernel_size=1)\n",
    "\n",
    "        # --- Motion Stream ---\n",
    "        self.motion_backbone = timm.create_model('mobilenetv3_small_050', pretrained=True, features_only=True, out_indices=[2])\n",
    "        # MobileNetV3-S stage 2 has 16 channels, feature map size 14x14\n",
    "        self.motion_feature_proj = nn.Conv2d(16, feature_dim, kernel_size=1)\n",
    "\n",
    "        # --- Graph Network ---\n",
    "        self.num_nodes_per_frame = 14 * 14 \n",
    "        # GATConv input is concatenated features (appearance + motion)\n",
    "        gnn_input_dim = feature_dim * 2\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            GATConv(gnn_input_dim, gnn_input_dim, heads=num_heads, dropout=dropout, concat=False)\n",
    "            for _ in range(gnn_layers)\n",
    "        ])\n",
    "        self.gnn_norm = nn.LayerNorm(gnn_input_dim)\n",
    "        \n",
    "        # --- Temporal Attention ---\n",
    "        self.temporal_attention = nn.MultiheadAttention(embed_dim=gnn_input_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # --- Classifier Head ---\n",
    "        self.fusion_norm = nn.LayerNorm(gnn_input_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gnn_input_dim, gnn_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(gnn_input_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb_clips, motion_clips):\n",
    "        batch_size, _, _, height, width = rgb_clips.shape\n",
    "        \n",
    "        # Reshape for batch processing: [B, T, C, H, W] -> [B*T, C, H, W]\n",
    "        rgb_reshaped = rgb_clips.view(-1, 3, height, width)\n",
    "        motion_reshaped = motion_clips.view(-1, 3, height, width)\n",
    "\n",
    "        # --- Feature Extraction ---\n",
    "        appearance_feat = self.cnn_backbone(rgb_reshaped)[0]\n",
    "        appearance_feat = self.cnn_feature_proj(appearance_feat)\n",
    "        \n",
    "        motion_feat = self.motion_backbone(motion_reshaped)[0]\n",
    "        motion_feat = self.motion_feature_proj(motion_feat)\n",
    "\n",
    "        # --- Graph Construction ---\n",
    "        combined_feat = torch.cat([appearance_feat, motion_feat], dim=1)\n",
    "        \n",
    "        _, d_combined, h_feat, w_feat = combined_feat.shape\n",
    "        # Reshape for GNN: [B*T, 2D, H', W'] -> [B, T, N, 2D] where N=H'*W'\n",
    "        graph_nodes = combined_feat.view(batch_size, self.num_frames, d_combined, -1).permute(0, 1, 3, 2)\n",
    "\n",
    "        # Create a fully-connected graph edge structure for each frame\n",
    "        edge_index = self._create_fully_connected_edges(h_feat * w_feat, graph_nodes.device)\n",
    "        \n",
    "        # Process each time step through the GNN\n",
    "        gnn_outputs = []\n",
    "        for t in range(self.num_frames):\n",
    "            frame_nodes = graph_nodes[:, t, :, :].reshape(-1, d_combined) # [B*N, 2D]\n",
    "            \n",
    "            # Create a batch of graphs for torch_geometric\n",
    "            batch_indices = torch.arange(batch_size, device=frame_nodes.device).repeat_interleave(h_feat * w_feat)\n",
    "            \n",
    "            x = frame_nodes\n",
    "            for layer in self.gnn_layers:\n",
    "                x = F.elu(layer(x, edge_index))\n",
    "            \n",
    "            x = self.gnn_norm(x)\n",
    "            x = x.view(batch_size, h_feat * w_feat, d_combined) # Reshape back to [B, N, 2D]\n",
    "            gnn_outputs.append(x)\n",
    "\n",
    "        # Stack GNN outputs along the time dimension: [B, T, N, 2D]\n",
    "        gnn_outputs = torch.stack(gnn_outputs, dim=1)\n",
    "\n",
    "        # --- Temporal Aggregation ---\n",
    "        # Pool nodes spatially (average pooling) before temporal attention\n",
    "        spatially_pooled_features = gnn_outputs.mean(dim=2) # [B, T, 2D]\n",
    "\n",
    "        # Apply temporal attention\n",
    "        temporal_features, _ = self.temporal_attention(spatially_pooled_features, spatially_pooled_features, spatially_pooled_features)\n",
    "        \n",
    "        # Aggregate across time (average pooling) to get a final clip representation\n",
    "        final_representation = temporal_features.mean(dim=1) # [B, 2D]\n",
    "        \n",
    "        # --- Classification ---\n",
    "        final_representation = self.fusion_norm(final_representation)\n",
    "        logits = self.classifier(final_representation)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def _create_fully_connected_edges(self, num_nodes, device):\n",
    "        # Cache edge_index to avoid re-computation\n",
    "        if not hasattr(self, '_edge_index') or self._edge_index.device != device:\n",
    "            edge_list = torch.combinations(torch.arange(num_nodes, device=device), r=2).t()\n",
    "            # Make it undirected by adding reverse edges\n",
    "            self._edge_index = torch.cat([edge_list, edge_list.flip(0)], dim=1)\n",
    "        return self._edge_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf4b4f-f63f-4732-8dd5-ca3fdd055b18",
   "metadata": {},
   "source": [
    "### TRAINING/VALIDATION LOOP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e652034b-150f-4245-95f1-3e09334afdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, optimizer, criterion, device, is_training=True):\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\" if is_training else \"Validation\")\n",
    "    \n",
    "    for rgb_frames, motion_frames, labels in progress_bar:\n",
    "        rgb_frames, motion_frames, labels = rgb_frames.to(device), motion_frames.to(device), labels.to(device)\n",
    "\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            logits = model(rgb_frames, motion_frames)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * rgb_frames.size(0)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += rgb_frames.size(0)\n",
    "        \n",
    "        acc = correct_predictions.double() / total_samples\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc:.4f}\")\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613c744-c16f-447a-8be1-954fb59c9c14",
   "metadata": {},
   "source": [
    "### MAIN EXECUTION BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3801bb53-60d7-466b-b922-47c273d172be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # 6. MAIN EXECUTION BLOCK\n",
    "# # ==============================================================================\n",
    "# if __name__ == '__main__':\n",
    "#     # --- Configuration ---\n",
    "    \n",
    "# config = {\n",
    "#     'data': {\n",
    "#         'root': 'video-anomaly-detection/data',\n",
    "#         'train_annotations': 'video-anomaly-detection/data/train_annotations.csv',\n",
    "#         'val_annotations': 'video-anomaly-detection/data/val_annotations.csv',\n",
    "#     },\n",
    "#     'model': {\n",
    "#         'num_frames': 16,\n",
    "#         'num_classes': 2, # Make sure this is 2 for your dataset\n",
    "#         'feature_dim': 128, \n",
    "#     },\n",
    "#     'train': {\n",
    "#         'batch_size': 4, \n",
    "#         'warmup_epochs': 3,\n",
    "#         'epochs': 10,\n",
    "#         'lr_heads': 3e-4,\n",
    "#         'lr_backbone': 1e-5,\n",
    "#         'weight_decay': 1e-2,\n",
    "#         'checkpoint_dir': 'video-anomaly-detection/weights'\n",
    "#     }\n",
    "# }\n",
    "#     # --- CHANGE THIS PART ---\n",
    "#     # OLD CODE:\n",
    "#     # generate_dummy_data() \n",
    "#     # OR\n",
    "#     # create_annotations_from_csvs('path/to/folder')\n",
    "\n",
    "#     # --- TO THIS ---\n",
    "#     # Define the path to the folder containing '1', '2', '3', etc.\n",
    "#     YOUR_DATASET_PATH = \"C://Users//srish//Downloads//movies\" # <--- CHANGE THIS PATH\n",
    "#     create_annotations_from_nested_folders(YOUR_DATASET_PATH)\n",
    "\n",
    "\n",
    "#     # --- AND CHANGE THIS PART ---\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "#     # --- DataLoaders ---\n",
    "#     # OLD CODE:\n",
    "#     # train_dataset = VideoDataset(annotations_file=config['data']['train_annotations'], data_root=config['data']['root'], ...)\n",
    "    \n",
    "#     # --- TO THIS ---\n",
    "#     # Make sure data_root points to your main dataset folder\n",
    "#     train_dataset = VideoDataset(annotations_file=config['data']['train_annotations'], data_root=YOUR_DATASET_PATH, is_train=True)\n",
    "#     val_dataset = VideoDataset(annotations_file=config['data']['val_annotations'], data_root=YOUR_DATASET_PATH, is_train=False)\n",
    "\n",
    "#     # ... (the rest of the script stays the same) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6000ae26-410b-478a-aadc-31d4f7b078fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for videos in nested folders under: C://Users//srish//Downloads//movies\n",
      "Annotation files created successfully from nested folders.\n",
      "Total videos found: 246\n",
      "Training set size: 196\n",
      "Validation set size: 50\n",
      "Using device: cpu\n",
      "\n",
      "--- Stage B: Warmup (Training Classifier Heads) ---\n",
      "\n",
      "Warmup Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/49 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 22388, 21036) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1285\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\multiprocessing\\queues.py:112\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWarmup Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m run_epoch(model, train_loader, optimizer, criterion, device, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  -> Warmup Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(model, val_loader, \u001b[38;5;28;01mNone\u001b[39;00m, criterion, device, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[52], line 14\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device, is_training)\u001b[0m\n\u001b[0;32m     10\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rgb_frames, motion_frames, labels \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     15\u001b[0m     rgb_frames, motion_frames, labels \u001b[38;5;241m=\u001b[39m rgb_frames\u001b[38;5;241m.\u001b[39mto(device), motion_frames\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1492\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1495\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1454\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1453\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1454\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1456\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1297\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1299\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1300\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 22388, 21036) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "# --- Configuration --- \n",
    "    config = {\n",
    "    'data': {\n",
    "        'root': 'video-anomaly-detection/data',\n",
    "        'train_annotations': 'video-anomaly-detection/data/train_annotations.csv',\n",
    "        'val_annotations': 'video-anomaly-detection/data/val_annotations.csv',\n",
    "    },\n",
    "    'model': {\n",
    "        'num_frames': 16,\n",
    "        'num_classes': 3,\n",
    "        'feature_dim': 128, # Reduced for faster training on Colab\n",
    "    },\n",
    "    'train': {\n",
    "        'batch_size': 4, \n",
    "        'warmup_epochs': 3,\n",
    "        'epochs': 10,\n",
    "        'lr_heads': 3e-4,\n",
    "        'lr_backbone': 1e-5, # Lower LR for fine-tuning\n",
    "        'weight_decay': 1e-2,\n",
    "        'checkpoint_dir': 'video-anomaly-detection/weights'\n",
    "    }\n",
    " }\n",
    " # --- TO THIS ---\n",
    "    # Define the path to the folder containing '1', '2', '3', etc.\n",
    "    YOUR_DATASET_PATH = \"C://Users//srish//Downloads//movies\" # <--- CHANGE THIS PATH\n",
    "    create_annotations_from_nested_folders(YOUR_DATASET_PATH)\n",
    "\n",
    "# Make sure data_root points to your main dataset folder\n",
    "    train_dataset = VideoDataset(annotations_file=config['data']['train_annotations'], data_root=YOUR_DATASET_PATH, is_train=True)\n",
    "    val_dataset = VideoDataset(annotations_file=config['data']['val_annotations'], data_root=YOUR_DATASET_PATH, is_train=False)\n",
    "\n",
    "\n",
    "# --- Main Logic ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset = VideoDataset(annotations_file=config['data']['train_annotations'], data_root=config['data']['root'])\n",
    "val_dataset = VideoDataset(annotations_file=config['data']['val_annotations'], data_root=config['data']['root'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['train']['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model = SpatioTemporalGNN(\n",
    "    num_frames=config['model']['num_frames'],\n",
    "    num_classes=config['model']['num_classes'],\n",
    "    feature_dim=config['model']['feature_dim']\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Stage B: Warmup\n",
    "print(\"\\n--- Stage B: Warmup (Training Classifier Heads) ---\")\n",
    "for param in model.cnn_backbone.parameters(): param.requires_grad = False\n",
    "for param in model.motion_backbone.parameters(): param.requires_grad = False\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config['train']['lr_heads'])\n",
    "\n",
    "for epoch in range(config['train']['warmup_epochs']):\n",
    "    print(f\"\\nWarmup Epoch {epoch+1}/{config['train']['warmup_epochs']}\")\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion, device, is_training=True)\n",
    "    print(f\"  -> Warmup Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, None, criterion, device, is_training=False)\n",
    "    print(f\"  -> Warmup Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Stage C: Fine-tuning\n",
    "print(\"\\n--- Stage C: Fine-tuning (End-to-End) ---\")\n",
    "for param in model.parameters(): param.requires_grad = True\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['train']['lr_backbone'], weight_decay=config['train']['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['train']['epochs'])\n",
    "\n",
    "for epoch in range(config['train']['epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['train']['epochs']}\")\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion, device, is_training=True)\n",
    "    print(f\"  -> Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, None, criterion, device, is_training=False)\n",
    "    print(f\"  -> Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), os.path.join(config['train']['checkpoint_dir'], 'best_model.pth'))\n",
    "        print(f\"  -> New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best validation accuracy achieved: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e07cfd-40c2-45d1-b472-c268dd52ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VideoDataset(Dataset):\n",
    "#     def __init__(self, annotations_file, data_root, num_frames=16, frame_size=(224, 224), fps=16):\n",
    "#         self.annotations = pd.read_csv(annotations_file)\n",
    "#         self.data_root = data_root\n",
    "#         self.num_frames = num_frames\n",
    "#         self.frame_size = frame_size\n",
    "#         self.target_fps = fps\n",
    "#         self.labels_map = {\"Normal\": 0, \"Panic\": 1, \"Violent\": 2}\n",
    "\n",
    "#         self.transform = A.Compose([\n",
    "#             A.RandomResizedCrop(height=self.frame_size[0], width=self.frame_size[1], scale=(0.8, 1.0)),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.8),\n",
    "#             A.GaussNoise(p=0.2),\n",
    "#             A.CoarseDropout(max_holes=8, max_height=16, max_width=16, min_holes=1, min_height=8, min_width=8, p=0.5),\n",
    "#             A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#             ToTensorV2(),\n",
    "#         ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         video_path = os.path.join(self.data_root, self.annotations.iloc[idx, 0])\n",
    "#         label_str = self.annotations.iloc[idx, 1]\n",
    "#         label = self.labels_map[label_str]\n",
    "\n",
    "#         frames, frame_diffs = self._load_video_clip(video_path)\n",
    "\n",
    "#         transformed_frames = torch.stack([self.transform(image=frame)[\"image\"] for frame in frames])\n",
    "        \n",
    "#         frame_diffs = torch.from_numpy(frame_diffs).float().permute(0, 3, 1, 2) # T, H, W, C -> T, C, H, W\n",
    "#         frame_diffs_normalized = torch.stack([A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])(image=d.permute(1,2,0).numpy())['image'] for d in frame_diffs])\n",
    "#         frame_diffs_tensor = ToTensorV2()(image=frame_diffs_normalized.permute(1,2,3,0))['image'].squeeze(0)\n",
    "\n",
    "#         return transformed_frames, frame_diffs_tensor.permute(3,0,1,2), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "#     def _load_video_clip(self, video_path):\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         if not cap.isOpened():\n",
    "#             raise Exception(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "#         frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "\n",
    "#         frames = []\n",
    "#         frame_diffs = []\n",
    "#         last_frame_gray = None\n",
    "\n",
    "#         loaded_frames = 0\n",
    "#         for i in range(total_frames):\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "            \n",
    "#             if i in frame_indices:\n",
    "#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                 frame = cv2.resize(frame, (self.frame_size[1], self.frame_size[0]))\n",
    "#                 frames.append(frame)\n",
    "#                 loaded_frames += 1\n",
    "\n",
    "#                 current_frame_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "#                 if last_frame_gray is not None:\n",
    "#                     diff = cv2.absdiff(current_frame_gray, last_frame_gray)\n",
    "#                     frame_diffs.append(np.stack([diff]*3, axis=-1))\n",
    "#                 last_frame_gray = current_frame_gray\n",
    "        \n",
    "#         cap.release()\n",
    "\n",
    "#         while len(frames) < self.num_frames:\n",
    "#             frames.append(frames[-1].copy())\n",
    "        \n",
    "#         if not frame_diffs:\n",
    "#             frame_diffs.append(np.zeros_like(frames[0]))\n",
    "\n",
    "#         while len(frame_diffs) < self.num_frames:\n",
    "#             frame_diffs.append(np.zeros_like(frame_diffs[-1]))\n",
    "\n",
    "#         return np.array(frames), np.array(frame_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091ca08-edd7-4496-91ad-03a78022b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SpatioTemporalGNN(nn.Module):\n",
    "#     def __init__(self, num_frames=16, num_classes=3, feature_dim=256, gnn_layers=2, num_heads=4, dropout=0.3):\n",
    "#         super().__init__()\n",
    "#         self.num_frames = num_frames\n",
    "#         self.feature_dim = feature_dim\n",
    "\n",
    "#         self.cnn_backbone = timm.create_model('convnext_tiny', pretrained=True, features_only=True, out_indices=[2])\n",
    "#         self.cnn_feature_proj = nn.Conv2d(384, feature_dim, kernel_size=1)\n",
    "\n",
    "#         self.motion_backbone = timm.create_model('mobilenetv3_small_050', pretrained=True, features_only=True, out_indices=[2])\n",
    "#         self.motion_feature_proj = nn.Conv2d(16, feature_dim, kernel_size=1)\n",
    "\n",
    "#         self.num_nodes_per_frame = 14 * 14 \n",
    "#         self.gnn_layers = nn.ModuleList([\n",
    "#             GATConv(feature_dim * 2, feature_dim * 2, heads=num_heads, dropout=dropout, concat=False)\n",
    "#             for _ in range(gnn_layers)\n",
    "#         ])\n",
    "        \n",
    "#         self.temporal_attention = nn.MultiheadAttention(embed_dim=feature_dim * 2, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "#         self.fusion_norm = nn.LayerNorm(feature_dim * 2)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(feature_dim * 2, feature_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(feature_dim, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, rgb_clips, motion_clips):\n",
    "#         batch_size, _, _, height, width = rgb_clips.shape\n",
    "        \n",
    "#         rgb_clips_reshaped = rgb_clips.view(batch_size * self.num_frames, 3, height, width)\n",
    "#         motion_clips_reshaped = motion_clips.view(batch_size * self.num_frames, 3, height, width)\n",
    "\n",
    "#         appearance_features = self.cnn_backbone(rgb_clips_reshaped)[0]\n",
    "#         appearance_features = self.cnn_feature_proj(appearance_features)\n",
    "        \n",
    "#         motion_features = self.motion_backbone(motion_clips_reshaped)[0]\n",
    "#         motion_features = self.motion_feature_proj(motion_features)\n",
    "\n",
    "#         combined_features = torch.cat([appearance_features, motion_features], dim=1)\n",
    "        \n",
    "#         _, d_combined, h_feat, w_feat = combined_features.shape\n",
    "#         self.num_nodes_per_frame = h_feat * w_feat\n",
    "#         graph_nodes = combined_features.view(batch_size * self.num_frames, d_combined, -1).permute(0, 2, 1)\n",
    "#         graph_nodes = graph_nodes.view(batch_size, self.num_frames, self.num_nodes_per_frame, d_combined)\n",
    "\n",
    "#         edge_index = self._create_fully_connected_edges(self.num_nodes_per_frame, graph_nodes.device)\n",
    "        \n",
    "#         gnn_outputs = []\n",
    "#         for t in range(self.num_frames):\n",
    "#             frame_nodes = graph_nodes[:, t, :, :]\n",
    "            \n",
    "#             pyg_data_list = [Data(x=frame_nodes[b], edge_index=edge_index) for b in range(batch_size)]\n",
    "#             pyg_batch = Batch.from_data_list(pyg_data_list)\n",
    "            \n",
    "#             x, edge_idx = pyg_batch.x, pyg_batch.edge_index\n",
    "#             for layer in self.gnn_layers:\n",
    "#                 x = F.elu(layer(x, edge_idx))\n",
    "            \n",
    "#             x = x.view(batch_size, self.num_nodes_per_frame, -1)\n",
    "#             gnn_outputs.append(x)\n",
    "\n",
    "#         gnn_outputs = torch.stack(gnn_outputs, dim=1)\n",
    "\n",
    "#         spatially_pooled_features = gnn_outputs.mean(dim=2)\n",
    "\n",
    "#         temporal_features, _ = self.temporal_attention(spatially_pooled_features, spatially_pooled_features, spatially_pooled_features)\n",
    "        \n",
    "#         final_representation = temporal_features.mean(dim=1)\n",
    "        \n",
    "#         final_representation = self.fusion_norm(final_representation)\n",
    "#         logits = self.classifier(final_representation)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "#     def _create_fully_connected_edges(self, num_nodes, device):\n",
    "#         # This can be pre-computed and stored\n",
    "#         if not hasattr(self, '_edge_index') or self._edge_index.device != device or self._edge_index_num_nodes != num_nodes:\n",
    "#             edge_list = torch.combinations(torch.arange(num_nodes, device=device), r=2).t()\n",
    "#             self._edge_index = torch.cat([edge_list, edge_list.flip(0)], dim=1)\n",
    "#             self._edge_index_num_nodes = num_nodes\n",
    "#         return self._edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49228a-34b0-4880-a40c-34c26d1185e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_epoch(model, dataloader, optimizer, criterion, device, is_training=True):\n",
    "#     if is_training:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "#     total_loss = 0.0\n",
    "#     correct_predictions = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     progress_bar = tqdm(dataloader, desc=\"Training\" if is_training else \"Validation\", leave=False)\n",
    "    \n",
    "#     for rgb_frames, motion_frames, labels in progress_bar:\n",
    "#         rgb_frames = rgb_frames.to(device)\n",
    "#         motion_frames = motion_frames.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         if is_training:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         with torch.set_grad_enabled(is_training):\n",
    "#             logits = model(rgb_frames, motion_frames)\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             if is_training:\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item() * rgb_frames.size(0)\n",
    "#         _, preds = torch.max(logits, 1)\n",
    "#         correct_predictions += torch.sum(preds == labels.data)\n",
    "#         total_samples += rgb_frames.size(0)\n",
    "        \n",
    "#         acc = correct_predictions.double() / total_samples\n",
    "#         progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc:.4f}\")\n",
    "\n",
    "#     epoch_loss = total_loss / total_samples\n",
    "#     epoch_acc = correct_predictions.double() / total_samples\n",
    "#     return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f734ce-3aaf-425a-bda1-33479d1f727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Configuration --- \n",
    "# config = {\n",
    "#     'data': {\n",
    "#         'root': 'video-anomaly-detection/data',\n",
    "#         'train_annotations': 'video-anomaly-detection/data/train_annotations.csv',\n",
    "#         'val_annotations': 'video-anomaly-detection/data/val_annotations.csv',\n",
    "#     },\n",
    "#     'model': {\n",
    "#         'num_frames': 16,\n",
    "#         'num_classes': 3,\n",
    "#         'feature_dim': 128, # Reduced for faster training on Colab\n",
    "#     },\n",
    "#     'train': {\n",
    "#         'batch_size': 4, \n",
    "#         'warmup_epochs': 3,\n",
    "#         'epochs': 10,\n",
    "#         'lr_heads': 3e-4,\n",
    "#         'lr_backbone': 1e-5, # Lower LR for fine-tuning\n",
    "#         'weight_decay': 1e-2,\n",
    "#         'checkpoint_dir': 'video-anomaly-detection/weights'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # --- Main Logic ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# train_dataset = VideoDataset(annotations_file=config['data']['train_annotations'], data_root=config['data']['root'])\n",
    "# val_dataset = VideoDataset(annotations_file=config['data']['val_annotations'], data_root=config['data']['root'])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config['train']['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# model = SpatioTemporalGNN(\n",
    "#     num_frames=config['model']['num_frames'],\n",
    "#     num_classes=config['model']['num_classes'],\n",
    "#     feature_dim=config['model']['feature_dim']\n",
    "# ).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# best_val_acc = 0.0\n",
    "\n",
    "# # Stage B: Warmup\n",
    "# print(\"\\n--- Stage B: Warmup (Training Classifier Heads) ---\")\n",
    "# for param in model.cnn_backbone.parameters(): param.requires_grad = False\n",
    "# for param in model.motion_backbone.parameters(): param.requires_grad = False\n",
    "# optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=config['train']['lr_heads'])\n",
    "\n",
    "# for epoch in range(config['train']['warmup_epochs']):\n",
    "#     print(f\"\\nWarmup Epoch {epoch+1}/{config['train']['warmup_epochs']}\")\n",
    "#     train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion, device, is_training=True)\n",
    "#     print(f\"  -> Warmup Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "#     val_loss, val_acc = run_epoch(model, val_loader, None, criterion, device, is_training=False)\n",
    "#     print(f\"  -> Warmup Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# # Stage C: Fine-tuning\n",
    "# print(\"\\n--- Stage C: Fine-tuning (End-to-End) ---\")\n",
    "# for param in model.parameters(): param.requires_grad = True\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=config['train']['lr_backbone'], weight_decay=config['train']['weight_decay'])\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['train']['epochs'])\n",
    "\n",
    "# for epoch in range(config['train']['epochs']):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{config['train']['epochs']}\")\n",
    "#     train_loss, train_acc = run_epoch(model, train_loader, optimizer, criterion, device, is_training=True)\n",
    "#     print(f\"  -> Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "#     val_loss, val_acc = run_epoch(model, val_loader, None, criterion, device, is_training=False)\n",
    "#     print(f\"  -> Val Loss  : {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "#     scheduler.step()\n",
    "\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         torch.save(model.state_dict(), os.path.join(config['train']['checkpoint_dir'], 'best_model.pth'))\n",
    "#         print(f\"  -> New best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# print(\"\\nTraining complete.\")\n",
    "# print(f\"Best validation accuracy achieved: {best_val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
